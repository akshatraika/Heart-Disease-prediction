{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Stochastic subgradient descent for soft-margin linear SVM classifier Tutorial\n",
    "\n",
    "*Updated:* May 2, 2020  \n",
    "\n",
    "*Julia version:* 1.3.1\n",
    "\n",
    "## INTRODUCTION:\n",
    "\n",
    "This notebook is a tutorial on data classification using a Soft Margin Linear Support Vector Machine (SVM) with Stochastic Subgradient Descent. \n",
    "\n",
    "An SVM is an algorithm for finding an optimal separating hyperplane in n-dimensional space that distinctly classifies data points in a dataset. The hyperplane divides an n-dimensional vector space into subspaces, where n is the number of features in the dataset.\n",
    "\n",
    "In this notebook, we will demonstrate an SVM performing a binary classification of data. The dataset we are using classifies people as having or not having developed heart disease over a ten year period (Attribute #16: TenYearCHD), based on the following remaining 15 attributes:\n",
    "\n",
    "     1. Male: 1 or 0\n",
    "     2. Age: age of patient at exam time\n",
    "     3. Education: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = college\n",
    "     4. CurrentSmoker: 1 or 0\n",
    "     5. CigsPerDay: number of cigarettes smoked per day (estimated average)\n",
    "     6. BPMeds: 1 = on Blood Pressure Medication, 0 = not on Blood Pressure Medication\n",
    "     7. PrevalentStroke: 1 or 0\n",
    "     8. PrevalentHyp: 1 = Hypertension, 0 = no\n",
    "     9. Diabetes: 1 or 0\n",
    "    10. TotChol: cholestoral level in mg/dL\n",
    "    11. SysBP: systolic Blood Pressure in mmHg\n",
    "    12. DiaBP: diastolic Blood Pressure in mmHg\n",
    "    13. BMI: Body Mass Index calculated as Weight (kg) / Height(meter-squared)\n",
    "    14. HeartRate: in Beats/Min (Ventricular)\n",
    "    15. Glucose: measured in mg/dL\n",
    "    16. TenYearCHD: 1 = heart disease, 0 = no heart disease\n",
    "\n",
    "\n",
    "## THEORY\n",
    "\n",
    "The outcome of a Support Vector Machine (SVM) is an optimal separating hyperplane that distinctly classifies each observation in the dataset based on its features. The hyperplane splits the vector space containing the data points into subspaces that define the classification of each data point in the subspace. Therefore, an optimal separating hyperplane is a hyperplane that is as far as possible from each observation in the dataset, while still correctly classifying the observations. A greater distance from observations to the hyperplane increases the confidence in the classification. \n",
    "\n",
    "\n",
    "There are two types of SVMs: hard margin and soft margin. Hard margin SVMs are used when the data is perfectly separable; that is, there exists a hyperplane that perfectly classifies all of the observations. Soft margin SVMs extend the hard margin problem for data that is not perfectly separable. \n",
    "\n",
    "\n",
    "While our demonstration is a soft margin problem, a hard margin SVM is easier to visualize and provides a good starting point for discussion of SVMs. \n",
    "\n",
    "\n",
    "Below is a visual representation of what a hard margin SVM binary classification in two dimensions looks like. The solution of this SVM is the optimal hyperplane depicted as the red line running through the middle of the data points. This hyperplane classifies the points as either green or blue based on what side of the line they lie on.\n",
    "\n",
    "![Support Vector Machine](SVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the optimal hyperplane (red) is halfway between two parallel hyperplanes running through the observations closest to the separating hyperplane. Instead of directly finding the optimal hyperplane, the hard margin SVM algorithm finds two parallel hyperplanes that are as far apart as possible, with no observations in between, and each hyperplane running through at least one observation. The optimal hyperplane is then placed an equal distance between these two parallel hyperplanes.  \n",
    "\n",
    "The equations for the two parallel hyperplanes are modified perceptrons, an alogrithm used in machine learning for supervised learning of binary classifiers. How these modified perceptions are used to fit an SVM depends on the type of SVM, hard margin or soft margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Margin SVM\n",
    "\n",
    "A hard margin SVM is used when the data is linearly separable. In a hard margin problem, for a normalized dataset, the algorithm finds two parallel hyperplanes, with at least one data point on each, that correctly divides the data. These hyperplanes define a boundary of thickness $\\frac{2}{||\\mathbf{w}||}$; the algorithm aims to minimize $||\\mathbf{w}||$ and so maximize the distance between the hyperplanes, called the margin. \n",
    "\n",
    "\n",
    "In an SVM used for binary classification, the two hyperplanes are defined by the equations:\n",
    "\n",
    "\n",
    "$$\\mathbf{w}.\\mathbf{x}_i - b = 1\\text{}\\quad$$\n",
    "$$\\mathbf{w}.\\mathbf{x}_i - b = -1\\text{}\\quad$$\n",
    "\n",
    "where $\\mathbf{w}$ is the normal vector to the hyperplanes, $\\mathbf{x}_i$ is the $i^{th}$ data point in the dataset and b is the offset from the origin. \n",
    "\n",
    "Thus, observations are classfied by:\n",
    "\n",
    "$$\\mathbf{w}.\\mathbf{x}_i + b \\geq 1\\text{, if}\\quad\\mathbf{y}_i = 1$$\n",
    "$$\\mathbf{w}.\\mathbf{x}_i + b \\leq -1\\text{, if}\\quad\\mathbf{y}_i = -1$$\n",
    "\n",
    "\n",
    "These equations can be simplified as:\n",
    "\n",
    "$$\\mathbf{y}_i.\\left(\\mathbf{w}.\\mathbf{x}_i + b = 1\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin SVM\n",
    "\n",
    "A soft margin SVM is used when the data cannot be perfectly separated by a single hyperplane. Most real-world applications have data that is linearly inseparable, so soft margin SVMs are important for application. \n",
    "\n",
    "The idea of soft margin SVMs is based on this premise: allow the SVM to make a certain number of mistakes in the classification process, while keeping the margin as wide as possible so that other points can still be classified correctly. This can be done simply by modifying the objective of the SVM. The SVM problem can be treated as a minimization problem where the minimization of $\\mathbf{w}$ must be balanced with the correct classification of data points. Below is the function we will use for our demonstration of soft margin SVM.\n",
    "\n",
    "\n",
    "![Soft Margin SVM](Soft_margin.png)\n",
    "\n",
    "In this function, $\\lambda$ is a constant that is used to determine the trade-off between increasing the margin size and ensuring that the $\\mathbf{x_i}$s lie on the correct side of the hyperplane.\n",
    "\n",
    "Minimizing the above expression will give the least erroneous division of the vector space to classify the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Subgradient Descent\n",
    "\n",
    "Descent methods are common methods used for solving minimization problems. These methods are iterative algorithms with the goal of finding the minimum of a function. Gradient descent is one such method that finds the local minimum of a differentiable function by taking \"steps\" proportional to the negative of the gradient of the function. \n",
    "\n",
    "We focus on two main types of gradient descent: Batch Gradient Descent and Stochastic Gradient Descent.\n",
    "\n",
    "Batch gradient descent computes the gradient for the entire dataset; that is, we first compute the gradient vector of our target minimization function for the whole dataset with respect to our parameter vector. For each interation of this method, the gradient of the function is calculated at the current point, and a step is taken proportional to the negative of that value. In the case of soft margin SVMs, $\\mathbf{w}$ is updated each iteration based on the gradient of the function $f(\\mathbf{w}^t) = max(0, 1 - y_i\\mathbf{w}^t, {x_i}\\rangle)$. As shown below:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{t+1}\n",
    "= \\mathbf{w}^t - \\alpha_t \\nabla f(\\mathbf{w}^t),\n",
    "\\quad t=0,1,2\\ldots\n",
    "$$\n",
    "\n",
    "for a sequence of steplengths $\\alpha_t > 0$. \n",
    "\n",
    "This algorithm is very slow, however, as it incorporates every data observation into the calculation. This is a major disadvantage of using batch gradient descent, and instead, many applications use stochastic gradient descent. \n",
    "\n",
    "Stochastic gradient descent chooses a point along the function at random (and with replacement) and calculates the gradient at that particular point. This value is then used to update $\\mathbf{w}$. This method decreases the runtime of the algorithm by using the advantages of stochastic processes to update the weights $\\mathbf{w}$ incrementally with a single training sample. This is in contrast to the batch gradient descent which updates the weights $\\mathbf{w}$ based on the sum of the accumulated errors over all the observation points.\n",
    "\n",
    "Below is a plot of the steps taken by stochastic gradient descent versus batch gradient descent.\n",
    "\n",
    "![Gradient Descent](gradient_descent.png)\n",
    "\n",
    "In some cases, the expression $\\mathbf{w}^t - \\alpha_t \\nabla f(\\mathbf{w}^t)$ is unable to perform the correct operations. This is typically the case when$\\ f$ is undifferentiable.\n",
    "\n",
    "We then turn to the subgradient descent gradient. This method simplifies $\\mathbf{w}^t - \\alpha_t \\nabla f(\\mathbf{w}^t)$\n",
    "for implementation with all functions. In our problem, we will use the following expression to calculate $\\mathbf{w}^{t+1}$:\n",
    "\n",
    "# $$\\mathbf{w}^{t+1} = \\frac{1}{\\lambda t}\\sum_{j=1}^{t} \\theta^{t} + y_i\\mathbf{x_i}$$\n",
    "\n",
    "\n",
    "where $\\theta$ is initialized to 0 and updated every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this notebook is dedicated to demonstrating the use of stochastic subgradient descent to create a soft margin SVM for classifying patient's likelihood of developing heart disease over a ten year period. The steps we will follow, as well as a sudo-code example, are below.  \n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Get the training data\n",
    "2. Normalize it\n",
    "3. Visualise the data\n",
    "4. Calculate the hyperplane using SGD (run the minimization problem with the following pseudo-code)\n",
    "5. Visualise the hyperplane found by SGD\n",
    "6. Use SVM to classify new data\n",
    "\n",
    "![Sudo code](sudo_code.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, StatsPlots, Statistics, Plots, LinearAlgebra;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>male</th><th>age</th><th>education</th><th>currentSmoker</th><th>cigsPerDay</th><th>BPMeds</th><th>prevalentStroke</th><th>prevalentHyp</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 16 columns (omitted printing of 8 columns)</p><tr><th>1</th><td>1</td><td>39</td><td>4.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>0</td><td>46</td><td>2.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>1</td><td>48</td><td>1.0</td><td>1</td><td>20.0</td><td>0.0</td><td>0</td><td>0</td></tr><tr><th>4</th><td>0</td><td>61</td><td>3.0</td><td>1</td><td>30.0</td><td>0.0</td><td>0</td><td>1</td></tr><tr><th>5</th><td>0</td><td>46</td><td>3.0</td><td>1</td><td>23.0</td><td>0.0</td><td>0</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& male & age & education & currentSmoker & cigsPerDay & BPMeds & prevalentStroke & prevalentHyp & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Int64 & Float64 & Float64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 39 & 4.0 & 0 & 0.0 & 0.0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 0 & 46 & 2.0 & 0 & 0.0 & 0.0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t3 & 1 & 48 & 1.0 & 1 & 20.0 & 0.0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t4 & 0 & 61 & 3.0 & 1 & 30.0 & 0.0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t5 & 0 & 46 & 3.0 & 1 & 23.0 & 0.0 & 0 & 0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×16 DataFrame. Omitted printing of 10 columns\n",
       "│ Row │ male  │ age   │ education │ currentSmoker │ cigsPerDay │ BPMeds  │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mInt64\u001b[39m         │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼───────┼───────┼───────────┼───────────────┼────────────┼─────────┤\n",
       "│ 1   │ 1     │ 39    │ 4.0       │ 0             │ 0.0        │ 0.0     │\n",
       "│ 2   │ 0     │ 46    │ 2.0       │ 0             │ 0.0        │ 0.0     │\n",
       "│ 3   │ 1     │ 48    │ 1.0       │ 1             │ 20.0       │ 0.0     │\n",
       "│ 4   │ 0     │ 61    │ 3.0       │ 1             │ 30.0       │ 0.0     │\n",
       "│ 5   │ 0     │ 46    │ 3.0       │ 1             │ 23.0       │ 0.0     │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from file and visualize a portion of the DataFrame\n",
    "df = CSV.read(\"./FraminghamClean.csv\")\n",
    "first(df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of observations in the dataset (the number of rows in the DataFrame)\n",
    "nrow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Real</th><th>Float64</th><th>Real</th><th>Nothing</th><th>Nothing</th><th>DataType</th></tr></thead><tbody><p>16 rows × 8 columns</p><tr><th>1</th><td>male</td><td>0.443685</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>2</th><td>age</td><td>49.5519</td><td>32</td><td>49.0</td><td>70</td><td></td><td></td><td>Int64</td></tr><tr><th>3</th><td>education</td><td>1.98032</td><td>1.0</td><td>2.0</td><td>4.0</td><td></td><td></td><td>Float64</td></tr><tr><th>4</th><td>currentSmoker</td><td>0.489065</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>5</th><td>cigsPerDay</td><td>9.02542</td><td>0.0</td><td>0.0</td><td>70.0</td><td></td><td></td><td>Float64</td></tr><tr><th>6</th><td>BPMeds</td><td>0.0303445</td><td>0.0</td><td>0.0</td><td>1.0</td><td></td><td></td><td>Float64</td></tr><tr><th>7</th><td>prevalentStroke</td><td>0.00574084</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>8</th><td>prevalentHyp</td><td>0.311646</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>9</th><td>diabetes</td><td>0.027064</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>10</th><td>totChol</td><td>236.848</td><td>113.0</td><td>234.0</td><td>600.0</td><td></td><td></td><td>Float64</td></tr><tr><th>11</th><td>sysBP</td><td>132.371</td><td>83.5</td><td>128.0</td><td>295.0</td><td></td><td></td><td>Float64</td></tr><tr><th>12</th><td>diaBP</td><td>82.917</td><td>48.0</td><td>82.0</td><td>142.5</td><td></td><td></td><td>Float64</td></tr><tr><th>13</th><td>BMI</td><td>25.7828</td><td>15.54</td><td>25.38</td><td>56.8</td><td></td><td></td><td>Float64</td></tr><tr><th>14</th><td>heartRate</td><td>75.7307</td><td>44.0</td><td>75.0</td><td>143.0</td><td></td><td></td><td>Float64</td></tr><tr><th>15</th><td>glucose</td><td>81.8529</td><td>40.0</td><td>78.0</td><td>394.0</td><td></td><td></td><td>Float64</td></tr><tr><th>16</th><td>TenYearCHD</td><td>0.152269</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Real & Float64 & Real & Nothing & Nothing & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & male & 0.443685 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t2 & age & 49.5519 & 32 & 49.0 & 70 &  &  & Int64 \\\\\n",
       "\t3 & education & 1.98032 & 1.0 & 2.0 & 4.0 &  &  & Float64 \\\\\n",
       "\t4 & currentSmoker & 0.489065 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t5 & cigsPerDay & 9.02542 & 0.0 & 0.0 & 70.0 &  &  & Float64 \\\\\n",
       "\t6 & BPMeds & 0.0303445 & 0.0 & 0.0 & 1.0 &  &  & Float64 \\\\\n",
       "\t7 & prevalentStroke & 0.00574084 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t8 & prevalentHyp & 0.311646 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t9 & diabetes & 0.027064 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t10 & totChol & 236.848 & 113.0 & 234.0 & 600.0 &  &  & Float64 \\\\\n",
       "\t11 & sysBP & 132.371 & 83.5 & 128.0 & 295.0 &  &  & Float64 \\\\\n",
       "\t12 & diaBP & 82.917 & 48.0 & 82.0 & 142.5 &  &  & Float64 \\\\\n",
       "\t13 & BMI & 25.7828 & 15.54 & 25.38 & 56.8 &  &  & Float64 \\\\\n",
       "\t14 & heartRate & 75.7307 & 44.0 & 75.0 & 143.0 &  &  & Float64 \\\\\n",
       "\t15 & glucose & 81.8529 & 40.0 & 78.0 & 394.0 &  &  & Float64 \\\\\n",
       "\t16 & TenYearCHD & 0.152269 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "16×8 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ variable        │ mean       │ min   │ median  │ max   │ nunique │\n",
       "│     │ \u001b[90mSymbol\u001b[39m          │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mReal\u001b[39m  │ \u001b[90mFloat64\u001b[39m │ \u001b[90mReal\u001b[39m  │ \u001b[90mNothing\u001b[39m │\n",
       "├─────┼─────────────────┼────────────┼───────┼─────────┼───────┼─────────┤\n",
       "│ 1   │ male            │ 0.443685   │ 0     │ 0.0     │ 1     │         │\n",
       "│ 2   │ age             │ 49.5519    │ 32    │ 49.0    │ 70    │         │\n",
       "│ 3   │ education       │ 1.98032    │ 1.0   │ 2.0     │ 4.0   │         │\n",
       "│ 4   │ currentSmoker   │ 0.489065   │ 0     │ 0.0     │ 1     │         │\n",
       "│ 5   │ cigsPerDay      │ 9.02542    │ 0.0   │ 0.0     │ 70.0  │         │\n",
       "│ 6   │ BPMeds          │ 0.0303445  │ 0.0   │ 0.0     │ 1.0   │         │\n",
       "│ 7   │ prevalentStroke │ 0.00574084 │ 0     │ 0.0     │ 1     │         │\n",
       "│ 8   │ prevalentHyp    │ 0.311646   │ 0     │ 0.0     │ 1     │         │\n",
       "│ 9   │ diabetes        │ 0.027064   │ 0     │ 0.0     │ 1     │         │\n",
       "│ 10  │ totChol         │ 236.848    │ 113.0 │ 234.0   │ 600.0 │         │\n",
       "│ 11  │ sysBP           │ 132.371    │ 83.5  │ 128.0   │ 295.0 │         │\n",
       "│ 12  │ diaBP           │ 82.917     │ 48.0  │ 82.0    │ 142.5 │         │\n",
       "│ 13  │ BMI             │ 25.7828    │ 15.54 │ 25.38   │ 56.8  │         │\n",
       "│ 14  │ heartRate       │ 75.7307    │ 44.0  │ 75.0    │ 143.0 │         │\n",
       "│ 15  │ glucose         │ 81.8529    │ 40.0  │ 78.0    │ 394.0 │         │\n",
       "│ 16  │ TenYearCHD      │ 0.152269   │ 0     │ 0.0     │ 1     │         │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the features of the data\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and save as matrix X\n",
    "X = reduce(hcat, [df[!,:male],df[!,:age],df[!,:education],df[!,:currentSmoker],df[!,:cigsPerDay],df[!,:BPMeds],\n",
    "        df[!,:prevalentStroke],df[!,:prevalentHyp],df[!,:diabetes],df[!,:totChol],df[!,:sysBP],df[!,:diaBP],\n",
    "        df[!,:BMI],df[!,:heartRate],df[!,:glucose]]);\n",
    "# Extract classifications and save as array Y\n",
    "Y_bin = df[!,16];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y (classifications) from 1s and 0s to 1s and -1s\n",
    "Y = zeros(length(Y_bin))\n",
    "for i in 1:length(Y_bin)\n",
    "    \n",
    "    if Y_bin[i] == 0\n",
    "        Y[i] = -1\n",
    "    \n",
    "    else\n",
    "        Y[i] = 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm_sgd (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function performs Stochastic Subgradient descent and returns an array of w's\n",
    "function svm_sgd(X, Y, T)\n",
    "    \n",
    "    # initialize theta and w to zero; learning rate lambda is 1\n",
    "    lambda = 1\n",
    "    theta = zeros(length(X[1,:]))\n",
    "    w = zeros(length(X[1,:]))\n",
    "    \n",
    "    # perform T iterations, updating theta and w every iteration\n",
    "    for t in 1:T\n",
    "        \n",
    "        w = (1/(lambda*t))*theta\n",
    "        i = rand(1:length(X[:,1]))\n",
    "        \n",
    "        if Y[i]*(w'X[i,:]) < 1\n",
    "            theta = theta .+ Y[i]*X[i,:]\n",
    "        else\n",
    "            theta = theta\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets with 70/30 split\n",
    "X_train = X[1:2560,:]\n",
    "Y_train = Y[1:2560]\n",
    "\n",
    "X_test = X[2561:3658,:]\n",
    "Y_test = Y[2561:3658];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Float64,1}:\n",
       "  0.008700000000000001 \n",
       "  0.0618               \n",
       " -0.0413               \n",
       " -0.0017000000000000001\n",
       "  0.0708               \n",
       "  0.0037               \n",
       "  0.0007               \n",
       "  0.0187               \n",
       "  0.004200000000000001 \n",
       " -0.019                \n",
       "  0.11125              \n",
       " -0.1005               \n",
       " -0.05315300000000001  \n",
       " -0.1459               \n",
       "  0.033800000000000004 "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run SGD for T=10000 iterations and display final array of w\n",
    "w = svm_sgd(X_train,Y_train,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424408014571949"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy of model by counting the number of correct classifications from the testing set\n",
    "truth = 0\n",
    "len = length(X_test[:,1])\n",
    "for i in 1:length(X_test[:,1])\n",
    "\n",
    "    Y_pred = X_test[i,:]'w\n",
    "    if Y_test[i] < 0\n",
    "        if Y_pred < 0\n",
    "            truth = truth + 1\n",
    "        end\n",
    "    else\n",
    "        if Y_pred > 0\n",
    "            truth = truth + 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "truth/len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\n",
    "\n",
    "2. https://en.wikipedia.org/wiki/Support-vector_machine?fbclid=IwAR0KxlzzHiWeUx8uI2LeCzV0dXlfIhtWidkBPt5r5QSOEdtYnIzg_ir6a5s#Sub-gradient_descent\n",
    "\n",
    "3. “Machine Learning with Scikit-Learn.” Scikit-Learn: Batch Gradient Descent versus Stochastic Gradient Descent - 2020, 2014, www.bogotobogo.com/python/scikit-learn/scikit-learn_batch-gradient-descent-versus-stochastic-gradient-descent.php.\n",
    "\n",
    "4. Shalev-Shwartz, Shai, et al. “Pegasos: Primal Estimated Sub-Gradient Solver for SVM.” Mathematical Programming, vol. 127, no. 1, 2010, pp. 3–30., doi:10.1007/s10107-010-0420-4.\n",
    "\n",
    "5. Theodoridis, Sergios. “Subgradient.” Subgradient - an Overview | ScienceDirect Topics, ScienceDirect, 2014, www.sciencedirect.com/topics/engineering/subgradient.\n",
    "\n",
    "6. Understanding Machine Learning: from Theory to Algorithms, by Shai Shalev-Shwartz and Shai Ben-David, Cambridge University Press, 2017, pp. 202–213."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
